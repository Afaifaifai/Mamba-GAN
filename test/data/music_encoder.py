# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
  
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#   http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from performance_event_repo import PerformanceEventRepo
import functools
import time
import os, sys
import pandas as pd
import logging
sys.path.append(os.path.dirname(sys.path[0]))
from utils import find_files_by_extensions
from tqdm import tqdm
import glob
from datasets import Dataset, DatasetDict

_CURR_DIR = os.path.realpath(os.path.dirname(os.path.realpath(__file__)))
MAESTOR_V1_DIR = os.path.join(_CURR_DIR, 'maestro-v1.0.0')

def read_maestro_meta_info(data_dir):
    """Read the meta information from Maestro

    Parameters
    ----------
    data_dir
        The base path of the maestro data

    Returns
    -------
    df
        Pandas Dataframe, with the following columns:
        ['canonical_composer',
         'canonical_title',
         'split',
         'year',
         'midi_filename',
         'audio_filename',
         'duration']
    """
    if os.path.exists(os.path.join(data_dir, 'maestro-v1.0.0.csv')):
        logging.info('Process maestro-v1.')
        csv_path = os.path.join(data_dir, 'maestro-v1.0.0.csv')
    elif os.path.exists(os.path.join(data_dir, 'maestro-v2.0.0.csv')):
        logging.info('Process maestro-v2.')
        csv_path = os.path.join(data_dir, 'maestro-v2.0.0.csv')
    else:
        raise ValueError('Cannot found valid csv files!')
    df = pd.read_csv(csv_path)
    return df


def get_midi_paths():
    if not os.path.exists(MAESTOR_V1_DIR):
        raise ValueError('Cannot find maestro-v1.0.0, use `get_data.sh` to download and '
                         'extract the data.')
    df = read_maestro_meta_info(MAESTOR_V1_DIR)
    train_paths = df[df['split'] == 'train']['midi_filename'].to_numpy()
    validation_paths = df[df['split'] == 'validation']['midi_filename'].to_numpy()
    test_paths = df[df['split'] == 'test']['midi_filename'].to_numpy()
    train_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in train_paths]
    validation_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in validation_paths]
    test_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in test_paths]
    return train_paths, validation_paths, test_paths


if __name__ == '__main__':

    from argparse import ArgumentParser
    import multiprocessing as mpl

    parser = ArgumentParser()
    parser.add_argument('--input_folder', type=str,
                        help='Directory with the downloaded MAESTOR dataset',
                        default=MAESTOR_V1_DIR)
    parser.add_argument('--output_folder', type=str,
                        help='Directory to encode the event signals', required=True)
    parser.add_argument('--encode_official_maestro', action='store_true',
                        help='Whether to encode the official Maestro dataset.')
    parser.add_argument('--mode', type=str,
                        help='Convert to/from MIDIs to TXT/Numpy',
                        choices=['to_txt', 'to_midi', 'txt_to_alltxt', 'npy_to_allnpy',
                                 'midi_to_npy', 'npy_to_midi'],
                        default='to_txt')
    parser.add_argument('--stretch_factors', type=str, help='Stretch Factors',
                        default='0.95,0.975,1.0,1.025,1.05')
    parser.add_argument('--pitch_transpose_lower', type=int,
                        help='Lower bound of the pitch transposition amounts',
                        default=-3)
    parser.add_argument('--pitch_transpose_upper', type=int,
                        help='Uppwer bound of the pitch transposition amounts',
                        default=3)
    args = parser.parse_args()


    stretch_factors = [float(ele) for ele in args.stretch_factors.split(',')]
    encoder = PerformanceEventRepo(steps_per_second=100, num_velocity_bins=32,
                               stretch_factors=stretch_factors,
                               pitch_transpose_lower=args.pitch_transpose_lower,
                               pitch_transpose_upper=args.pitch_transpose_upper)

    def run_to_text(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_text(path, os.path.join(out_dir, filename + '.txt'))


    def run_to_text_with_transposition(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_text_transposition(path, os.path.join(out_dir, filename + '.txt'))


    def run_to_npy(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_npy(path, os.path.join(out_dir, filename + '.npy'))


    def run_to_npy_with_transposition(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_npy_transposition(path, os.path.join(out_dir, filename + '.npy'))


    def run_from_text(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.from_text(path, os.path.join(out_dir, filename + '.mid'))


    def run_npy_to_midi(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.npy_to_midi(path, os.path.join(out_dir, filename + '.mid'))

    def merge_txt_files_in_folder(source_folder, output_file):
        """
        æƒæä¸€å€‹ä¾†æºè³‡æ–™å¤¾ï¼Œæ‰¾åˆ°å…¶ä¸­æ‰€æœ‰çš„ .txt æª”æ¡ˆï¼Œ
        å°‡æ¯å€‹æª”æ¡ˆçš„å…§å®¹åˆä½µæˆä¸€è¡Œï¼Œç„¶å¾Œå…¨éƒ¨å¯«å…¥ä¸€å€‹å¤§çš„è¼¸å‡ºæª”æ¡ˆã€‚

        Args:
            source_folder (str): åŒ…å«å¤šå€‹ .txt æª”æ¡ˆçš„ä¾†æºè³‡æ–™å¤¾è·¯å¾‘ã€‚
            output_file (str): åˆä½µå¾Œè¦å„²å­˜çš„å–®ä¸€æª”æ¡ˆè·¯å¾‘ã€‚
        """
        # ä½¿ç”¨ glob æ‰¾åˆ°æ‰€æœ‰ .txt æª”æ¡ˆçš„è·¯å¾‘
        txt_files = glob.glob(os.path.join(source_folder, '*.txt'))
        
        if not txt_files:
            print(f"è­¦å‘Šï¼šåœ¨ '{source_folder}' ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• .txt æª”æ¡ˆã€‚")
            return

        print(f"æ‰¾åˆ° {len(txt_files)} å€‹ .txt æª”æ¡ˆã€‚é–‹å§‹å¾ '{source_folder}' åˆä½µè‡³ '{output_file}'...")
        
        # æ‰“é–‹è¼¸å‡ºçš„æª”æ¡ˆæº–å‚™å¯«å…¥
        with open(output_file, 'w', encoding='utf-8') as f_out:
            # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
            for file_path in tqdm(txt_files, desc=f"åˆä½µ {os.path.basename(source_folder)}"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f_in:
                        # è®€å–æª”æ¡ˆä¸­çš„æ‰€æœ‰è¡Œ (æ¯å€‹ token)ï¼Œä¸¦ç§»é™¤æ›è¡Œç¬¦
                        tokens = [line.strip() for line in f_in.readlines()]
                        # å°‡æ‰€æœ‰ token ç”¨ç©ºæ ¼é€£æ¥æˆä¸€å€‹é•·å­—ä¸²
                        sequence_line = " ".join(tokens)
                        # å°‡é€™å€‹ä»£è¡¨å–®ä¸€åºåˆ—çš„é•·å­—ä¸²å¯«å…¥è¼¸å‡ºæª”æ¡ˆï¼Œä¸¦åŠ ä¸Šæ›è¡Œç¬¦
                        f_out.write(sequence_line + "\n")
                except Exception as e:
                    print(f"è™•ç†æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
        print(f"æˆåŠŸï¼'{source_folder}' ä¸­çš„æ‰€æœ‰ .txt æª”æ¡ˆå·²åˆä½µè‡³ '{output_file}'ã€‚")

    def load_sequences_from_npy_folder(folder_path: str) -> list:
        """
        å¾ä¸€å€‹è³‡æ–™å¤¾ä¸­è¼‰å…¥æ‰€æœ‰ .npy æª”æ¡ˆï¼Œä¸¦å›å‚³ä¸€å€‹åŒ…å«æ‰€æœ‰åºåˆ—çš„ Python åˆ—è¡¨ã€‚
        
        Args:
            folder_path (str): åŒ…å« .npy æª”æ¡ˆçš„ä¾†æºè³‡æ–™å¤¾è·¯å¾‘ã€‚
            
        Returns:
            list: ä¸€å€‹åˆ—è¡¨ï¼Œå…¶ä¸­æ¯å€‹å…ƒç´ æ˜¯å¦ä¸€å€‹ä»£è¡¨ token ID åºåˆ—çš„åˆ—è¡¨ã€‚
        """
        # çµ„åˆå‡ºæœå°‹æ¨¡å¼ï¼Œä¾‹å¦‚: /path/to/train/*.npy
        search_pattern = os.path.join(folder_path, '*.npy')
        npy_files = glob.glob(search_pattern)

        if not npy_files:
            print(f"è­¦å‘Šï¼šåœ¨ '{folder_path}' ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• .npy æª”æ¡ˆã€‚å°‡å›å‚³ç©ºåˆ—è¡¨ã€‚")
            return []

        all_sequences = []
        # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
        for file_path in tqdm(npy_files, desc=f"å¾ {os.path.basename(folder_path)} è¼‰å…¥ npy"):
            try:
                # è¼‰å…¥ .npy æª”æ¡ˆä¸¦è½‰æ›ç‚º Python list
                sequence = np.load(file_path).tolist()
                all_sequences.append(sequence)
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                
        return all_sequences


    def convert_npy_splits_to_arrow_dataset(base_npy_dir: str, arrow_save_path: str):
        """
        è®€å–åŒ…å« train/valid/test å­è³‡æ–™å¤¾çš„ .npy æ•¸æ“šæ ¹ç›®éŒ„ï¼Œ
        å°‡å®ƒå€‘è½‰æ›ä¸¦åˆä½µæˆä¸€å€‹ Hugging Face Arrow æ•¸æ“šé›†ã€‚

        Args:
            base_npy_dir (str): åŒ…å« train/, valid/, test/ .npy å­è³‡æ–™å¤¾çš„æ ¹ç›®éŒ„ã€‚
            arrow_save_path (str): æ‚¨å¸Œæœ›å„²å­˜æœ€çµ‚ Arrow æ•¸æ“šé›†çš„ç›®æ¨™è³‡æ–™å¤¾è·¯å¾‘ã€‚
        """
        print(f"é–‹å§‹å¾ .npy æª”æ¡ˆå»ºç«‹ Hugging Face Dataset...")
        print(f"ä¾†æºè³‡æ–™å¤¾: {base_npy_dir}")
        print(f"ç›®æ¨™ Arrow è³‡æ–™å¤¾: {arrow_save_path}")

        # å®šç¾©è¦è™•ç†çš„æ•¸æ“šåˆ†å‰²
        splits = ['train', 'validation', 'test']
        dataset_dict_content = {}

        # éæ­·æ¯ä¸€å€‹åˆ†å‰² (train, validation, test)
        for split_name in splits:
            # ç‰¹åˆ¥è™•ç† 'validation' åç¨±çš„å°æ‡‰
            folder_name = 'valid' if split_name == 'validation' else split_name
            source_folder = os.path.join(base_npy_dir, folder_name)
            
            # å‘¼å«è¼”åŠ©å‡½å¼ä¾†è¼‰å…¥è©²åˆ†å‰²çš„æ‰€æœ‰åºåˆ—
            sequences = load_sequences_from_npy_folder(source_folder)
            
            # å°‡åºåˆ—åˆ—è¡¨è½‰æ›æˆ Dataset ç‰©ä»¶
            # `datasets` å‡½å¼åº«æœŸæœ›çš„æ ¼å¼æ˜¯ {'æ¬„ä½å': [è³‡æ–™åˆ—è¡¨]}
            dataset = Dataset.from_dict({'input_ids': sequences})
            
            # å°‡è™•ç†å¥½çš„ Dataset ç‰©ä»¶å­˜å…¥å­—å…¸
            dataset_dict_content[split_name] = dataset

        # å°‡åŒ…å«æ‰€æœ‰åˆ†å‰²çš„å­—å…¸ï¼Œè½‰æ›æˆä¸€å€‹å®Œæ•´çš„ DatasetDict ç‰©ä»¶
        final_dataset = DatasetDict(dataset_dict_content)

        print("\næ•¸æ“šé›†çµæ§‹é è¦½:")
        print(final_dataset)
        
        # å°‡é€™å€‹æ•¸æ“šé›†å­—å…¸ä»¥é«˜æ•ˆçš„ Arrow æ ¼å¼å„²å­˜åˆ°ç¡¬ç¢Ÿ
        print(f"\næ­£åœ¨å°‡æ•¸æ“šé›†å„²å­˜è‡³ '{arrow_save_path}'...")
        os.makedirs(arrow_save_path, exist_ok=True)
        final_dataset.save_to_disk(arrow_save_path)

        print("-" * 50)
        print(f"ğŸ‰ æˆåŠŸï¼æ‚¨çš„ .npy æ•¸æ“šå·²è½‰æ›ç‚º Arrow æ•¸æ“šé›†ã€‚")
        print(f"å„²å­˜ä½ç½®: '{arrow_save_path}'")
        print("-" * 50)

    num_cpus = mpl.cpu_count()
    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)

    if args.mode == 'to_txt' or args.mode == 'midi_to_npy':
        if args.mode == 'to_txt':
            converted_format = 'txt'
            convert_transposition_f = run_to_text_with_transposition
            convert_f = run_to_text
        else:
            converted_format = 'npy'
            convert_transposition_f = run_to_npy_with_transposition
            convert_f = run_to_npy

        print('Converting midi files from {} to {}...'
              .format(args.input_folder, converted_format))
        if args.encode_official_maestro:
            train_paths, valid_paths, test_paths = get_midi_paths()
            print('Load MAESTRO V1 from {}. Train/Val/Test={}/{}/{}'
                  .format(args.input_folder, len(train_paths), len(valid_paths),
                          len(test_paths)))
            for split_name, midi_paths in [('train', train_paths),
                                           ('valid', valid_paths),
                                           ('test', test_paths)]:
                if split_name == 'train':
                    convert_function = convert_transposition_f
                else:
                    convert_function = convert_f
                out_split_dir = os.path.join(args.output_folder, split_name)
                os.makedirs(out_split_dir, exist_ok=True)
                start = time.time()
                with mpl.Pool(num_cpus - 1) as pool:
                    pool.map(functools.partial(convert_function, out_dir=out_split_dir),
                             midi_paths)
                print('Split {} converted! Spent {}s to convert {} samples.'
                      .format(split_name, time.time() - start, len(midi_paths)))
            encoder.create_vocab_txt(args.output_folder)
        else:
            midi_paths = []
            for root, _, files in os.walk(args.input_folder):
                for fname in files:
                    filename, extension = os.path.splitext(os.path.basename(fname))
                    if extension == '.mid' or extension == '.midi':
                        midi_paths.append(os.path.join(root, fname))
            os.makedirs(args.output_folder, exist_ok=True)
            start = time.time()
            with mpl.Pool(num_cpus - 1) as pool:
                pool.map(functools.partial(convert_f, out_dir=args.output_folder),
                         midi_paths)
            print('Converted midi files from {} to {}! Spent {}s to convert {} samples.'
                  .format(args.input_folder, args.output_folder,
                          time.time() - start, len(midi_paths)))
    elif args.mode == 'to_midi' or args.mode == 'npy_to_midi':
        convert_f = run_from_text if args.mode == 'to_midi' else run_npy_to_midi
        start = time.time()
        if args.mode == 'npy_to_midi':
            input_paths = list(find_files_by_extensions(args.input_folder, ['.npy']))
        else:
            input_paths = list(find_files_by_extensions(args.input_folder, ['.txt']))
        with mpl.Pool(num_cpus - 1) as pool:
            pool.map(functools.partial(convert_f,
                                       out_dir=args.output_folder),
                     input_paths)
        print('Test converted! Spent {}s to convert {} samples.'
              .format(time.time() - start, len(input_paths)))
    elif args.mode == 'txt_to_alltxt':
        BASE_DATA_DIR = "./maestro_magenta_s5_t3" 

        # --- å®šç¾©ä¾†æºè³‡æ–™å¤¾å’Œç›®æ¨™æª”æ¡ˆ ---
        # key æ˜¯ä¾†æºè³‡æ–™å¤¾çš„åç¨±ï¼Œvalue æ˜¯è¦è¼¸å‡ºçš„ç›®æ¨™æª”æ¡ˆåç¨±
        split_config = {
            "train": "train_all_data.txt",
            "valid": "valid_all_data.txt",
            "test": "test_all_data.txt"
        }

        # --- é–‹å§‹åŸ·è¡Œåˆä½µ ---
        for split_name, output_filename in split_config.items():
            # çµ„åˆå‡ºå®Œæ•´çš„ä¾†æºè³‡æ–™å¤¾è·¯å¾‘
            source_dir = os.path.join(BASE_DATA_DIR, split_name)
            # çµ„åˆå‡ºå®Œæ•´çš„è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            output_path = os.path.join(BASE_DATA_DIR, output_filename)
            
            # å‘¼å«åˆä½µå‡½å¼
            merge_txt_files_in_folder(source_dir, output_path)
            print("-" * 50)

        print("æ‰€æœ‰æ•¸æ“šé›†åˆä½µå®Œæˆï¼")

    elif args.mode == 'npy_to_allnpy':
        # 1. æŒ‡å®šåŒ…å« train/, valid/, test/ .npy å­è³‡æ–™å¤¾çš„æ ¹ç›®éŒ„
        #    (é€™æ˜¯æ‚¨åŸ·è¡Œå®Œ midi_to_npy æ¨¡å¼å¾Œç”¢ç”Ÿçš„è³‡æ–™å¤¾)
        SOURCE_NPY_ROOT = "./maestro_magenta_s5_t3"

        # 2. æŒ‡å®šæ‚¨å¸Œæœ›å„²å­˜æœ€çµ‚ Arrow æ•¸æ“šé›†çš„ç›®æ¨™è³‡æ–™å¤¾
        ARROW_OUTPUT_PATH = "./arrow_dataset"

        # 3. åŸ·è¡Œè½‰æ›å‡½å¼
        convert_npy_splits_to_arrow_dataset(
            base_npy_dir=SOURCE_NPY_ROOT,
            arrow_save_path=ARROW_OUTPUT_PATH
        )

    else:
        raise NotImplementedError