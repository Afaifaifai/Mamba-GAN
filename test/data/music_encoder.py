# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
  
# Licensed under the Apache License, Version 2.0 (the "License").
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#   http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from performance_event_repo import PerformanceEventRepo
import functools
import time
import os, sys
import pandas as pd
import logging
sys.path.append(os.path.dirname(sys.path[0]))
from utils import find_files_by_extensions
from tqdm import tqdm
import glob
from datasets import Dataset, DatasetDict

_CURR_DIR = os.path.realpath(os.path.dirname(os.path.realpath(__file__)))
MAESTOR_V1_DIR = os.path.join(_CURR_DIR, 'maestro-v1.0.0')

def read_maestro_meta_info(data_dir):
    """Read the meta information from Maestro

    Parameters
    ----------
    data_dir
        The base path of the maestro data

    Returns
    -------
    df
        Pandas Dataframe, with the following columns:
        ['canonical_composer',
         'canonical_title',
         'split',
         'year',
         'midi_filename',
         'audio_filename',
         'duration']
    """
    if os.path.exists(os.path.join(data_dir, 'maestro-v1.0.0.csv')):
        logging.info('Process maestro-v1.')
        csv_path = os.path.join(data_dir, 'maestro-v1.0.0.csv')
    elif os.path.exists(os.path.join(data_dir, 'maestro-v2.0.0.csv')):
        logging.info('Process maestro-v2.')
        csv_path = os.path.join(data_dir, 'maestro-v2.0.0.csv')
    else:
        raise ValueError('Cannot found valid csv files!')
    df = pd.read_csv(csv_path)
    return df


def get_midi_paths():
    if not os.path.exists(MAESTOR_V1_DIR):
        raise ValueError('Cannot find maestro-v1.0.0, use `get_data.sh` to download and '
                         'extract the data.')
    df = read_maestro_meta_info(MAESTOR_V1_DIR)
    train_paths = df[df['split'] == 'train']['midi_filename'].to_numpy()
    validation_paths = df[df['split'] == 'validation']['midi_filename'].to_numpy()
    test_paths = df[df['split'] == 'test']['midi_filename'].to_numpy()
    train_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in train_paths]
    validation_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in validation_paths]
    test_paths = [os.path.join(MAESTOR_V1_DIR, ele) for ele in test_paths]
    return train_paths, validation_paths, test_paths


if __name__ == '__main__':

    from argparse import ArgumentParser
    import multiprocessing as mpl

    parser = ArgumentParser()
    parser.add_argument('--input_folder', type=str,
                        help='Directory with the downloaded MAESTOR dataset',
                        default=MAESTOR_V1_DIR)
    parser.add_argument('--output_folder', type=str,
                        help='Directory to encode the event signals', required=True)
    parser.add_argument('--encode_official_maestro', action='store_true',
                        help='Whether to encode the official Maestro dataset.')
    parser.add_argument('--mode', type=str,
                        help='Convert to/from MIDIs to TXT/Numpy',
                        choices=['to_txt', 'to_midi', 'txt_to_alltxt', 'npy_to_allnpy',
                                 'midi_to_npy', 'npy_to_midi'],
                        default='to_txt')
    parser.add_argument('--stretch_factors', type=str, help='Stretch Factors',
                        default='0.95,0.975,1.0,1.025,1.05')
    parser.add_argument('--pitch_transpose_lower', type=int,
                        help='Lower bound of the pitch transposition amounts',
                        default=-3)
    parser.add_argument('--pitch_transpose_upper', type=int,
                        help='Uppwer bound of the pitch transposition amounts',
                        default=3)
    args = parser.parse_args()


    stretch_factors = [float(ele) for ele in args.stretch_factors.split(',')]
    encoder = PerformanceEventRepo(steps_per_second=100, num_velocity_bins=32,
                               stretch_factors=stretch_factors,
                               pitch_transpose_lower=args.pitch_transpose_lower,
                               pitch_transpose_upper=args.pitch_transpose_upper)

    def run_to_text(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_text(path, os.path.join(out_dir, filename + '.txt'))


    def run_to_text_with_transposition(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_text_transposition(path, os.path.join(out_dir, filename + '.txt'))


    def run_to_npy(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_npy(path, os.path.join(out_dir, filename + '.npy'))


    def run_to_npy_with_transposition(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.to_npy_transposition(path, os.path.join(out_dir, filename + '.npy'))


    def run_from_text(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.from_text(path, os.path.join(out_dir, filename + '.mid'))


    def run_npy_to_midi(path, out_dir):
        filename, extension = os.path.splitext(os.path.basename(path))
        encoder.npy_to_midi(path, os.path.join(out_dir, filename + '.mid'))

    def merge_txt_files_in_folder(source_folder, output_file):
        """
        æƒæä¸€å€‹ä¾†æºè³‡æ–™å¤¾ï¼Œæ‰¾åˆ°å…¶ä¸­æ‰€æœ‰çš„ .txt æª”æ¡ˆï¼Œ
        å°‡æ¯å€‹æª”æ¡ˆçš„å…§å®¹åˆä½µæˆä¸€è¡Œï¼Œç„¶å¾Œå…¨éƒ¨å¯«å…¥ä¸€å€‹å¤§çš„è¼¸å‡ºæª”æ¡ˆã€‚

        Args:
            source_folder (str): åŒ…å«å¤šå€‹ .txt æª”æ¡ˆçš„ä¾†æºè³‡æ–™å¤¾è·¯å¾‘ã€‚
            output_file (str): åˆä½µå¾Œè¦å„²å­˜çš„å–®ä¸€æª”æ¡ˆè·¯å¾‘ã€‚
        """
        # ä½¿ç”¨ glob æ‰¾åˆ°æ‰€æœ‰ .txt æª”æ¡ˆçš„è·¯å¾‘
        txt_files = glob.glob(os.path.join(source_folder, '*.txt'))
        
        if not txt_files:
            print(f"è­¦å‘Šï¼šåœ¨ '{source_folder}' ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• .txt æª”æ¡ˆã€‚")
            return

        print(f"æ‰¾åˆ° {len(txt_files)} å€‹ .txt æª”æ¡ˆã€‚é–‹å§‹å¾ '{source_folder}' åˆä½µè‡³ '{output_file}'...")
        
        # æ‰“é–‹è¼¸å‡ºçš„æª”æ¡ˆæº–å‚™å¯«å…¥
        with open(output_file, 'w', encoding='utf-8') as f_out:
            # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
            for file_path in tqdm(txt_files, desc=f"åˆä½µ {os.path.basename(source_folder)}"):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f_in:
                        # è®€å–æª”æ¡ˆä¸­çš„æ‰€æœ‰è¡Œ (æ¯å€‹ token)ï¼Œä¸¦ç§»é™¤æ›è¡Œç¬¦
                        tokens = [line.strip() for line in f_in.readlines()]
                        # å°‡æ‰€æœ‰ token ç”¨ç©ºæ ¼é€£æ¥æˆä¸€å€‹é•·å­—ä¸²
                        sequence_line = " ".join(tokens)
                        # å°‡é€™å€‹ä»£è¡¨å–®ä¸€åºåˆ—çš„é•·å­—ä¸²å¯«å…¥è¼¸å‡ºæª”æ¡ˆï¼Œä¸¦åŠ ä¸Šæ›è¡Œç¬¦
                        f_out.write(sequence_line + "\n")
                except Exception as e:
                    print(f"è™•ç†æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    
        print(f"æˆåŠŸï¼'{source_folder}' ä¸­çš„æ‰€æœ‰ .txt æª”æ¡ˆå·²åˆä½µè‡³ '{output_file}'ã€‚")

    def load_sequences_from_npy_folder(folder_path):
        """å¾ä¸€å€‹è³‡æ–™å¤¾ä¸­è¼‰å…¥æ‰€æœ‰ .npy æª”æ¡ˆï¼Œä¸¦å›å‚³ä¸€å€‹åŒ…å«æ‰€æœ‰åºåˆ—çš„åˆ—è¡¨ã€‚"""
        npy_files = glob.glob(os.path.join(folder_path, '*.npy'))
        all_sequences = []
        print(f"åœ¨ '{folder_path}' ä¸­æ‰¾åˆ° {len(npy_files)} å€‹ .npy æª”æ¡ˆ...")
        for file_path in tqdm(npy_files, desc=f"è¼‰å…¥ {os.path.basename(folder_path)}"):
            try:
                # è¼‰å…¥ .npy æª”æ¡ˆï¼Œä¸¦å°‡å…¶è½‰æ›ç‚º list
                sequence = np.load(file_path).tolist()
                all_sequences.append(sequence)
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return all_sequences

    num_cpus = mpl.cpu_count()
    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)

    if args.mode == 'to_txt' or args.mode == 'midi_to_npy':
        if args.mode == 'to_txt':
            converted_format = 'txt'
            convert_transposition_f = run_to_text_with_transposition
            convert_f = run_to_text
        else:
            converted_format = 'npy'
            convert_transposition_f = run_to_npy_with_transposition
            convert_f = run_to_npy

        print('Converting midi files from {} to {}...'
              .format(args.input_folder, converted_format))
        if args.encode_official_maestro:
            train_paths, valid_paths, test_paths = get_midi_paths()
            print('Load MAESTRO V1 from {}. Train/Val/Test={}/{}/{}'
                  .format(args.input_folder, len(train_paths), len(valid_paths),
                          len(test_paths)))
            for split_name, midi_paths in [('train', train_paths),
                                           ('valid', valid_paths),
                                           ('test', test_paths)]:
                if split_name == 'train':
                    convert_function = convert_transposition_f
                else:
                    convert_function = convert_f
                out_split_dir = os.path.join(args.output_folder, split_name)
                os.makedirs(out_split_dir, exist_ok=True)
                start = time.time()
                with mpl.Pool(num_cpus - 1) as pool:
                    pool.map(functools.partial(convert_function, out_dir=out_split_dir),
                             midi_paths)
                print('Split {} converted! Spent {}s to convert {} samples.'
                      .format(split_name, time.time() - start, len(midi_paths)))
            encoder.create_vocab_txt(args.output_folder)
        else:
            midi_paths = []
            for root, _, files in os.walk(args.input_folder):
                for fname in files:
                    filename, extension = os.path.splitext(os.path.basename(fname))
                    if extension == '.mid' or extension == '.midi':
                        midi_paths.append(os.path.join(root, fname))
            os.makedirs(args.output_folder, exist_ok=True)
            start = time.time()
            with mpl.Pool(num_cpus - 1) as pool:
                pool.map(functools.partial(convert_f, out_dir=args.output_folder),
                         midi_paths)
            print('Converted midi files from {} to {}! Spent {}s to convert {} samples.'
                  .format(args.input_folder, args.output_folder,
                          time.time() - start, len(midi_paths)))
    elif args.mode == 'to_midi' or args.mode == 'npy_to_midi':
        convert_f = run_from_text if args.mode == 'to_midi' else run_npy_to_midi
        start = time.time()
        if args.mode == 'npy_to_midi':
            input_paths = list(find_files_by_extensions(args.input_folder, ['.npy']))
        else:
            input_paths = list(find_files_by_extensions(args.input_folder, ['.txt']))
        with mpl.Pool(num_cpus - 1) as pool:
            pool.map(functools.partial(convert_f,
                                       out_dir=args.output_folder),
                     input_paths)
        print('Test converted! Spent {}s to convert {} samples.'
              .format(time.time() - start, len(input_paths)))
    elif args.mode == 'txt_to_alltxt':
        BASE_DATA_DIR = "./maestro_magenta_s5_t3" 

        # --- å®šç¾©ä¾†æºè³‡æ–™å¤¾å’Œç›®æ¨™æª”æ¡ˆ ---
        # key æ˜¯ä¾†æºè³‡æ–™å¤¾çš„åç¨±ï¼Œvalue æ˜¯è¦è¼¸å‡ºçš„ç›®æ¨™æª”æ¡ˆåç¨±
        split_config = {
            "train": "train_all_data.txt",
            "valid": "valid_all_data.txt",
            "test": "test_all_data.txt"
        }

        # --- é–‹å§‹åŸ·è¡Œåˆä½µ ---
        for split_name, output_filename in split_config.items():
            # çµ„åˆå‡ºå®Œæ•´çš„ä¾†æºè³‡æ–™å¤¾è·¯å¾‘
            source_dir = os.path.join(BASE_DATA_DIR, split_name)
            # çµ„åˆå‡ºå®Œæ•´çš„è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            output_path = os.path.join(BASE_DATA_DIR, output_filename)
            
            # å‘¼å«åˆä½µå‡½å¼
            merge_txt_files_in_folder(source_dir, output_path)
            print("-" * 50)

        print("æ‰€æœ‰æ•¸æ“šé›†åˆä½µå®Œæˆï¼")

    elif args.mode == 'npy_to_allnpy':
        # --- è«‹è¨­å®šæ‚¨çš„ä¾†æºå’Œç›®æ¨™è·¯å¾‘ ---
        # åŒ…å« train, valid, test ä¸‰å€‹å­è³‡æ–™å¤¾çš„ .npy æ•¸æ“šæ ¹ç›®éŒ„
        # é€™æ‡‰è©²æ˜¯æ‚¨åŸå§‹ music_encoder.py ç”¢ç”Ÿ .npy çš„åœ°æ–¹
        BASE_NPY_DIR = "./maestro_magenta_s5_t3" 
        
        # æ‚¨æƒ³è¦å°‡æœ€çµ‚è™•ç†å¥½çš„ Arrow æ ¼å¼æ•¸æ“šé›†å„²å­˜åˆ°å“ªè£¡
        FINAL_DATASET_SAVE_PATH = "./arrow_dataset"
        # ------------------------------------

        print("é–‹å§‹å¾ .npy æª”æ¡ˆå»ºç«‹ Hugging Face Dataset...")

        # åˆ†åˆ¥ç‚º train, valid, test å»ºç«‹ Dataset ç‰©ä»¶
        train_sequences = load_sequences_from_npy_folder(os.path.join(BASE_NPY_DIR, 'train'))
        valid_sequences = load_sequences_from_npy_folder(os.path.join(BASE_NPY_DIR, 'valid'))
        test_sequences = load_sequences_from_npy_folder(os.path.join(BASE_NPY_DIR, 'test'))

        # å°‡è¼‰å…¥çš„åºåˆ—è½‰æ›æˆ Dataset è¦æ±‚çš„å­—å…¸æ ¼å¼
        train_dataset = Dataset.from_dict({"input_ids": train_sequences})
        valid_dataset = Dataset.from_dict({"input_ids": valid_sequences})
        test_dataset = Dataset.from_dict({"input_ids": test_sequences})

        # å°‡ä¸‰å€‹ Dataset ç‰©ä»¶æ‰“åŒ…æˆä¸€å€‹ DatasetDict
        raw_datasets = DatasetDict({
            'train': train_dataset,
            'validation': valid_dataset,
            'test': test_dataset
        })

        print("\næ•¸æ“šé›†çµæ§‹é è¦½:")
        print(raw_datasets)
        
        # å°‡é€™å€‹æ•¸æ“šé›†å­—å…¸ä»¥é«˜æ•ˆçš„ Arrow æ ¼å¼å„²å­˜åˆ°ç¡¬ç¢Ÿ
        print(f"\næ­£åœ¨å°‡æ•¸æ“šé›†å„²å­˜è‡³ '{FINAL_DATASET_SAVE_PATH}'...")
        raw_datasets.save_to_disk(FINAL_DATASET_SAVE_PATH)

        print("-" * 50)
        print("ğŸ‰ æˆåŠŸï¼æ‚¨çš„ .npy æ•¸æ“šå·²è½‰æ›ç‚ºé«˜æ•ˆçš„ Arrow æ•¸æ“šé›†ã€‚")
        print(f"ç¾åœ¨æ‚¨å¯ä»¥åœ¨è¨“ç·´æŒ‡ä»¤ä¸­ä½¿ç”¨ '{FINAL_DATASET_SAVE_PATH}' é€™å€‹è·¯å¾‘äº†ã€‚")
        print("-" * 50)

    else:
        raise NotImplementedError